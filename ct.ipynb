{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VEEQV5z7MrGO"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip3 install tensorflow\n",
        "!pip3 install nibabel\n",
        "!pip3 install scipy\n",
        "!pip3 install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "from tensorflow import keras\n",
        "from scipy import ndimage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download url of normal CT scans and abnormal scans.\n",
        "\n",
        "# url = \"https://github.com/hasibzunair/3D-image-classification-tutorial/releases/download/v0.2/CT-0.zip\"\n",
        "# filename = os.path.join(os.getcwd(), \"CT-0.zip\")\n",
        "# keras.utils.get_file(filename, url)\n",
        "\n",
        "# url = \"https://github.com/hasibzunair/3D-image-classification-tutorial/releases/download/v0.2/CT-23.zip\"\n",
        "# filename = os.path.join(os.getcwd(), \"CT-23.zip\")\n",
        "# keras.utils.get_file(filename, url)\n",
        "\n",
        "if not os.path.exists(\"./CTData\"):\n",
        "    os.makedirs(\"CTData\")\n",
        "\n",
        "    with zipfile.ZipFile(\"CT-0.zip\", \"r\") as z_fp:\n",
        "        z_fp.extractall(\"./CTData/\")\n",
        "\n",
        "    with zipfile.ZipFile(\"CT-23.zip\", \"r\") as z_fp:\n",
        "        z_fp.extractall(\"./CTData/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_nifti_file(filepath):\n",
        "    # Read and load volume\n",
        "    scan = nib.load(filepath)\n",
        "    scan = scan.get_fdata()\n",
        "    return scan\n",
        "\n",
        "def normalize(volume):\n",
        "    # Normalize the volume\n",
        "    min = -1000\n",
        "    max = 400\n",
        "    volume[volume < min] = min\n",
        "    volume[volume > max] = max\n",
        "    volume = (volume - min) / (max - min)\n",
        "    volume = volume.astype(\"float32\")\n",
        "    return volume\n",
        "\n",
        "def resize_volume(img):\n",
        "    # Resize across z-axis\n",
        "    # Set the desired depth\n",
        "    desired_depth = 64\n",
        "    desired_width = 128\n",
        "    desired_height = 128\n",
        "    # Get current depth\n",
        "    current_depth = img.shape[-1]\n",
        "    current_width = img.shape[0]\n",
        "    current_height = img.shape[1]\n",
        "    # Compute depth factor\n",
        "    depth = current_depth / desired_depth\n",
        "    width = current_width / desired_width\n",
        "    height = current_height / desired_height\n",
        "    depth_factor = 1 / depth\n",
        "    width_factor = 1 / width\n",
        "    height_factor = 1 / height\n",
        "    # Rotate\n",
        "    img = ndimage.rotate(img, 90, reshape = False)\n",
        "    # Resize across z-axis\n",
        "    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order = 1)\n",
        "    return img\n",
        "\n",
        "\n",
        "def process_scan(path):\n",
        "    # Read and resize volume\n",
        "    volume = read_nifti_file(path)\n",
        "    volume = normalize(volume)\n",
        "    volume = resize_volume(volume)\n",
        "    return volume"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CT scans with normal lung tissue: 100\n",
            "CT scans with abnormal lung tissue: 100\n"
          ]
        }
      ],
      "source": [
        "normal_scan_paths = [\n",
        "    os.path.join(os.getcwd(), \"CTData/CT-0\", x)\n",
        "    for x in os.listdir(\"CTData/CT-0\")\n",
        "]\n",
        "abnormal_scan_paths = [\n",
        "    os.path.join(os.getcwd(), \"CTData/CT-23\", x)\n",
        "    for x in os.listdir(\"CTData/CT-23\")\n",
        "]\n",
        "\n",
        "print(\"CT scans with normal lung tissue: \" + str(len(normal_scan_paths)))\n",
        "print(\"CT scans with abnormal lung tissue: \" + str(len(abnormal_scan_paths)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pre-processing took 531.35 seconds.\n"
          ]
        }
      ],
      "source": [
        "processTime = time.time()\n",
        "\n",
        "abnormal_scans = np.array([process_scan(path) for path in abnormal_scan_paths])\n",
        "normal_scans = np.array([process_scan(path) for path in normal_scan_paths])\n",
        "\n",
        "abnormal_labels = np.array([1 for _ in range(len(abnormal_scans))])\n",
        "normal_labels = np.array([0 for _ in range(len(normal_scans))])\n",
        "\n",
        "print(\"Pre-processing took {} seconds.\".format(round(time.time() - processTime, 2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# both dataset should have equal samples.\n",
        "def split_datasets(split, dataset1, dataset2, label1, label2):\n",
        "    permutation = np.random.permutation(range(len(label1)))\n",
        "    split = int(len(label1) * split)\n",
        "    dataset1, label1 = dataset1[permutation,:,:,:], label1[permutation]\n",
        "    dataset2, label2 = dataset2[permutation,:,:,:], label2[permutation]\n",
        "    x_train = np.concatenate((dataset1[:split], dataset2[:split]), axis=0)\n",
        "    y_train = np.concatenate((label1[:split], label2[:split]), axis=0)\n",
        "    x_val = np.concatenate((dataset1[split:], dataset2[split:]), axis=0)\n",
        "    y_val = np.concatenate((label1[split:], label2[split:]), axis=0)\n",
        "    return x_train, y_train, x_val, y_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train, y_train, x_val, y_val = split_datasets(0.7, \n",
        "normal_scans, abnormal_scans, normal_labels, abnormal_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "trainData = []\n",
        "valData = []\n",
        "\n",
        "for i in range(x_train.shape[0]):\n",
        "  x = np.expand_dims(x_train[i].transpose(2, 0, 1), axis=0)\n",
        "  trainData.append((x, y_train[i]))\n",
        "\n",
        "for i in range(x_val.shape[0]):\n",
        "  x = np.expand_dims(x_val[i].transpose(2, 0, 1), axis=0)\n",
        "  valData.append((x, y_val[i]))\n",
        "\n",
        "trainDataLoader = DataLoader(trainData, shuffle=True, batch_size=3)\n",
        "valDataLoader = DataLoader(valData, batch_size=3)\n",
        "\n",
        "trainSteps = len(trainDataLoader.dataset) // 3 + 1\n",
        "valSteps = len(valDataLoader.dataset) // 3 + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "from torch import nn\n",
        "import generate_model\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(epoch, model, device, lossFn, optimizer, history):\n",
        "\tprint(\"[INFO] training the network...\")\n",
        "\tstartTime = time.time()\n",
        "\thistory[\"train_loss\"] = []\n",
        "\thistory[\"train_acc\"] = []\n",
        "\thistory[\"val_loss\"] = []\n",
        "\thistory[\"val_acc\"] = []\n",
        "\tfor e in range(0, epoch):\n",
        "\t\tmodel.train()\n",
        "\t\ttotalTrainLoss = 0\n",
        "\t\ttotalValLoss = 0\n",
        "\t\ttrainCorrect = 0\n",
        "\t\tvalCorrect = 0\n",
        "\t\tfor (x, y) in trainDataLoader:\n",
        "\t\t\t(x, y) = (x.to(device), y.to(device))\n",
        "\t\t\toptimizer.zero_grad()\n",
        "\t\t\tpred = model(x)\n",
        "\t\t\tloss = lossFn(pred, y)\n",
        "\t\t\tloss.backward()\n",
        "\t\t\toptimizer.step()\n",
        "\t\t\ttotalTrainLoss += loss\n",
        "\t\t\ttrainCorrect += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tmodel.eval()\n",
        "\t\t\tfor (x, y) in valDataLoader:\n",
        "\t\t\t\t(x, y) = (x.to(device), y.to(device))\n",
        "\t\t\t\tpred = model(x)\n",
        "\t\t\t\ttotalValLoss += lossFn(pred, y)\n",
        "\t\t\t\tvalCorrect += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "\t\tavgTrainLoss = totalTrainLoss / trainSteps\n",
        "\t\tavgValLoss = totalValLoss / valSteps\n",
        "\t\ttrainCorrect = trainCorrect / len(trainDataLoader.dataset)\n",
        "\t\tvalCorrect = valCorrect / len(valDataLoader.dataset)\n",
        "\t\thistory[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
        "\t\thistory[\"train_acc\"].append(trainCorrect)\n",
        "\t\thistory[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
        "\t\thistory[\"val_acc\"].append(valCorrect)\n",
        "\t\tprint(\"[INFO] EPOCH: {}/{}\".format(e + 1, epoch))\n",
        "\t\tprint(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(\n",
        "\t\t\tavgTrainLoss, trainCorrect))\n",
        "\t\tprint(\"Val loss: {:.6f}, Val accuracy: {:.4f}\\n\".format(\n",
        "\t\t\tavgValLoss, valCorrect))\n",
        "\tprint(\"[INFO] total time taken to train the model: {}s\".format(\n",
        "\t\tround(time.time() - startTime, 2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] training the network...\n",
            "[INFO] EPOCH: 1/50\n",
            "Train loss: 4.860854, Train accuracy: 0.4929\n",
            "Val loss: 1020349120512.000000, Val accuracy: 0.5000\n",
            "\n",
            "[INFO] EPOCH: 2/50\n",
            "Train loss: 3.561986, Train accuracy: 0.4857\n",
            "Val loss: 4928930.500000, Val accuracy: 0.5000\n",
            "\n",
            "[INFO] EPOCH: 3/50\n",
            "Train loss: 3.105308, Train accuracy: 0.4857\n",
            "Val loss: 184170545152.000000, Val accuracy: 0.5000\n",
            "\n",
            "[INFO] EPOCH: 4/50\n",
            "Train loss: 2.766868, Train accuracy: 0.4857\n",
            "Val loss: 3890922389504.000000, Val accuracy: 0.5000\n",
            "\n",
            "[INFO] EPOCH: 5/50\n",
            "Train loss: 2.514093, Train accuracy: 0.4929\n",
            "Val loss: 173850496.000000, Val accuracy: 0.5000\n",
            "\n",
            "[INFO] EPOCH: 6/50\n",
            "Train loss: 2.163941, Train accuracy: 0.5357\n",
            "Val loss: 199002.031250, Val accuracy: 0.5000\n",
            "\n",
            "[INFO] EPOCH: 7/50\n",
            "Train loss: 1.917966, Train accuracy: 0.4857\n",
            "Val loss: 36952.820312, Val accuracy: 0.5000\n",
            "\n",
            "[INFO] EPOCH: 8/50\n",
            "Train loss: 1.562223, Train accuracy: 0.5429\n",
            "Val loss: 935.209717, Val accuracy: 0.5000\n",
            "\n",
            "[INFO] EPOCH: 9/50\n",
            "Train loss: 1.605515, Train accuracy: 0.4857\n",
            "Val loss: 135.831314, Val accuracy: 0.4167\n",
            "\n",
            "[INFO] EPOCH: 10/50\n",
            "Train loss: 1.703465, Train accuracy: 0.5143\n",
            "Val loss: 961.648987, Val accuracy: 0.5000\n",
            "\n",
            "[INFO] EPOCH: 11/50\n",
            "Train loss: 1.520177, Train accuracy: 0.5429\n",
            "Val loss: 0.826642, Val accuracy: 0.4500\n",
            "\n",
            "[INFO] EPOCH: 12/50\n",
            "Train loss: 1.721831, Train accuracy: 0.4571\n",
            "Val loss: 1132.803711, Val accuracy: 0.5000\n",
            "\n",
            "[INFO] EPOCH: 13/50\n",
            "Train loss: 1.716696, Train accuracy: 0.4857\n",
            "Val loss: 1450.179932, Val accuracy: 0.5000\n",
            "\n",
            "[INFO] EPOCH: 14/50\n",
            "Train loss: 1.408254, Train accuracy: 0.4500\n",
            "Val loss: 1791.782715, Val accuracy: 0.5000\n",
            "\n",
            "[INFO] EPOCH: 15/50\n",
            "Train loss: 1.149409, Train accuracy: 0.5214\n",
            "Val loss: 8203.171875, Val accuracy: 0.5000\n",
            "\n",
            "[INFO] EPOCH: 16/50\n",
            "Train loss: 1.213531, Train accuracy: 0.4857\n",
            "Val loss: 33984.945312, Val accuracy: 0.5000\n",
            "\n",
            "[INFO] EPOCH: 17/50\n",
            "Train loss: 1.660708, Train accuracy: 0.5571\n",
            "Val loss: 5970.877441, Val accuracy: 0.5000\n",
            "\n",
            "[INFO] EPOCH: 18/50\n",
            "Train loss: 1.360859, Train accuracy: 0.4571\n",
            "Val loss: 1419.712036, Val accuracy: 0.5000\n",
            "\n",
            "[INFO] EPOCH: 19/50\n",
            "Train loss: 1.143586, Train accuracy: 0.4786\n",
            "Val loss: 422.767120, Val accuracy: 0.5000\n",
            "\n",
            "[INFO] EPOCH: 20/50\n",
            "Train loss: 1.078301, Train accuracy: 0.5786\n",
            "Val loss: 16.529819, Val accuracy: 0.6167\n",
            "\n",
            "[INFO] EPOCH: 21/50\n",
            "Train loss: 1.129448, Train accuracy: 0.4929\n",
            "Val loss: 102.460052, Val accuracy: 0.6333\n",
            "\n",
            "[INFO] EPOCH: 22/50\n",
            "Train loss: 1.092227, Train accuracy: 0.5286\n",
            "Val loss: 25.528166, Val accuracy: 0.5000\n",
            "\n",
            "[INFO] EPOCH: 23/50\n",
            "Train loss: 1.358234, Train accuracy: 0.4643\n",
            "Val loss: 2.350134, Val accuracy: 0.5000\n",
            "\n",
            "[INFO] EPOCH: 24/50\n",
            "Train loss: 1.135020, Train accuracy: 0.5000\n",
            "Val loss: 4.876718, Val accuracy: 0.7333\n",
            "\n",
            "[INFO] EPOCH: 25/50\n",
            "Train loss: 0.957801, Train accuracy: 0.5571\n",
            "Val loss: 12.397038, Val accuracy: 0.7333\n",
            "\n",
            "[INFO] EPOCH: 26/50\n",
            "Train loss: 1.102522, Train accuracy: 0.5143\n",
            "Val loss: 28.906250, Val accuracy: 0.2833\n",
            "\n",
            "[INFO] EPOCH: 27/50\n",
            "Train loss: 1.149099, Train accuracy: 0.4714\n",
            "Val loss: 4.010645, Val accuracy: 0.5000\n",
            "\n",
            "[INFO] EPOCH: 28/50\n",
            "Train loss: 1.127285, Train accuracy: 0.4143\n",
            "Val loss: 14.212896, Val accuracy: 0.7167\n",
            "\n",
            "[INFO] EPOCH: 29/50\n",
            "Train loss: 0.947120, Train accuracy: 0.5000\n",
            "Val loss: 35.163700, Val accuracy: 0.5000\n",
            "\n",
            "[INFO] EPOCH: 30/50\n",
            "Train loss: 0.894468, Train accuracy: 0.5143\n",
            "Val loss: 337.022766, Val accuracy: 0.5000\n",
            "\n",
            "[INFO] EPOCH: 31/50\n",
            "Train loss: 0.980284, Train accuracy: 0.4571\n",
            "Val loss: 5.071838, Val accuracy: 0.5000\n",
            "\n",
            "[INFO] EPOCH: 32/50\n",
            "Train loss: 1.069373, Train accuracy: 0.5429\n",
            "Val loss: 5.459167, Val accuracy: 0.4833\n",
            "\n",
            "[INFO] EPOCH: 33/50\n",
            "Train loss: 1.070311, Train accuracy: 0.4929\n",
            "Val loss: 4.195311, Val accuracy: 0.5000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = generate_model.main('EfficientNet', 10, 2, 1, None)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "lossFn = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr = 0.03)\n",
        "history = {}\n",
        "\n",
        "train_model(50, model, device, lossFn, optimizer, history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !jupyter nbconvert --to script ct.ipynb"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
